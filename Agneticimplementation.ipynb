{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTcRfYvqthvV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langgraph==0.2.45 langchain-google-genai==2.0.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "Tng4oULbvFDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "ruCSy1advFf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "5-yRhbb3vIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain.tools import Tool\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "# âœ… Set up Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"  # Replace with your actual key\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# âœ… Load Gemini Model in LangChain\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "\n",
        "# âœ… Define State for Medical Chatbot\n",
        "class MedicalChatState(TypedDict):\n",
        "    \"\"\"State representation of the medical chatbot session.\"\"\"\n",
        "    messages: Annotated[list, add_messages]  # Stores chat history\n",
        "    symptoms: list[str]  # Stores symptoms reported by user\n",
        "    diagnoses: list[str]  # Stores possible diagnoses\n",
        "    finished: bool  # Indicates if the session is complete\n",
        "\n",
        "# âœ… System Instructions for Chatbot\n",
        "MEDBOT_SYSTINT = AIMessage(\n",
        "    content=(\n",
        "        \"You are MedBot, an AI medical assistant. You help users by answering health-related queries, \"\n",
        "        \"analyzing symptoms, and providing preliminary diagnoses. \"\n",
        "        \"You do NOT prescribe medications or provide treatment recommendations. \"\n",
        "        \"For serious conditions, always advise users to consult a medical professional.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "WELCOME_MSG = \"Welcome to MedBot! Please describe your symptoms or ask a medical question.\"\n"
      ],
      "metadata": {
        "id": "DmG9vMMPtkUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph,START,END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "def chatbot(state:MedicalChatState)->MedicalChatState:\n",
        "  \"\"\"The chatbot itself.A simple wrapper around model's own chat interface\"\"\"\n",
        "  message_history=[MEDBOT_SYSTINT]+state[\"messages\"]\n",
        "  return {\"messages\": [llm.invoke(message_history)]}\n",
        "\n",
        "graph_builder=StateGraph(MedicalChatState)\n",
        "graph_builder.add_node(\"medicalchatbot\",chatbot)\n",
        "\n",
        "graph_builder.add_edge(START,\"medicalchatbot\")\n",
        "\n",
        "chat_graph=graph_builder.compile()\n"
      ],
      "metadata": {
        "id": "vsQOusITtiyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "mermaid_code = chat_graph.get_graph().draw_mermaid()\n",
        "display(Markdown(f\"```mermaid\\n{mermaid_code}\\n```\"))\n"
      ],
      "metadata": {
        "id": "pDtgciZtttZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "user_msg=\"Hello,what can you do?\"\n",
        "state=chat_graph.invoke({\"messages\":[user_msg]})\n",
        "for msg in state[\"messages\"]:\n",
        "   print(f\"{type(msg).__name__}:{msg.content}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "t4nKQ-_ytxWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.tools import tool\n",
        "from langchain.tools import Tool\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict"
      ],
      "metadata": {
        "id": "QrDKuXd2tyXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/processed_medquad.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(f\"âŒ Dataset file not found: {csv_path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "df = df.dropna(subset=[\"Question_Sentences\", \"Answer_Sentences\"])  # Drop missing values\n",
        "\n",
        "# âœ… Convert rows to LangChain Document objects\n",
        "docs_to_add = [\n",
        "    Document(\n",
        "        page_content=str(row[\"Question_Sentences\"]).strip(),\n",
        "        metadata={\n",
        "            \"answer\": str(row[\"Answer_Sentences\"]).strip(),\n",
        "            \"qtype\": str(row[\"qtype\"]).strip(),\n",
        "        }\n",
        "    )\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "\n",
        "# âœ… Load Hugging Face Sentence Transformer Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# âœ… Initialize Chroma Vector Store\n",
        "chroma_db_path = \"./chroma_db\"\n",
        "vector_store = Chroma(persist_directory=chroma_db_path, embedding_function=embedding_model)\n",
        "\n",
        "# âœ… Add Documents to Vector Store (only if empty)\n",
        "if len(vector_store.get()[\"ids\"]) == 0:\n",
        "    vector_store.add_documents(docs_to_add)\n",
        "    print(\"âœ… Medical QA dataset added to Chroma vector store!\")\n",
        "else:\n",
        "    print(\"âœ… Chroma vector store already populated.\")\n"
      ],
      "metadata": {
        "id": "vbdN7ecJt2ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "bn50lmsMt428"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "def human_node(state:MedicalChatState ) ->MedicalChatState:\n",
        "  last_msg=state[\"messages\"][-1]\n",
        "  print(\"Model\",last_msg.content)\n",
        "\n",
        "  user_input=input(\"User: \")\n",
        "\n",
        "  if user_input in {\"q\",\"quit\",\"goodbye\"}:\n",
        "    state[\"finished\"] =True\n",
        "\n",
        "  return state| {\"messages\":[(\"user\",user_input)]}\n",
        "\n",
        "def chatbot_with_welcome_msg(state: MedicalChatState) -> MedicalChatState:\n",
        "  if  state[\"messages\"]:\n",
        "    new_output=llm.invoke([MEDBOT_SYSTINT]+state[\"messages\"])\n",
        "  else:\n",
        "    new_output=AIMessage(content=WELCOME_MSG)\n",
        "\n",
        "  return state | {\"messages\":[new_output]}\n",
        "\n",
        "\n",
        "graph_builder=StateGraph(MedicalChatState)\n",
        "\n",
        "graph_builder.add_node(\"chatbot\",chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\",human_node)\n",
        "\n",
        "graph_builder.add_edge(START,\"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\",\"human\");\n"
      ],
      "metadata": {
        "id": "Va5kKIT-t9Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "def maybe_exit_human_node(state:MedicalChatState) ->Literal[\"chatbot\",\"__end__\"]:\n",
        "  if state.get(\"finished\",False):\n",
        "    return END\n",
        "  else:\n",
        "    return \"chatbot\"\n",
        "\n",
        "# Move these lines outside the function to make them global\n",
        "graph_builder.add_conditional_edges(\"human\",maybe_exit_human_node)\n",
        "chat_with_human_graph=graph_builder.compile()"
      ],
      "metadata": {
        "id": "AZOjtrNGuK0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state=chat_with_human_graph.invoke({\"messages\":[]})"
      ],
      "metadata": {
        "id": "bL2Z7UxWuLcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.llms import OpenAI  # Only if you use it later\n",
        "from langchain.schema import HumanMessage, AIMessage, BaseMessage\n",
        "from typing import Literal, Annotated, TypedDict, List\n",
        "\n",
        "# Define the MedicalChatState schema\n",
        "class MedicalChatState(TypedDict):\n",
        "    messages: List[BaseMessage]\n",
        "    finished: bool\n",
        "\n",
        "# âœ… Load pre-trained embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# âœ… Load the existing Chroma vector store\n",
        "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
        "\n",
        "# âœ… Tool for retrieving medical answers\n",
        "@tool\n",
        "def retrieve_medical_answer(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant medical answer from the Chroma vector store.\n",
        "    If no relevant answer is found, returns an empty response.\n",
        "    \"\"\"\n",
        "    results = vector_store.similarity_search(query, k=1)\n",
        "    if results:\n",
        "        best_match = results[0].metadata.get(\"answer\", \"\").strip()\n",
        "        if best_match:\n",
        "            return f\"ğŸ”¹ **Best Matched Answer:** {best_match}\"\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# âœ… Human node for user input\n",
        "def human_node(state: MedicalChatState) -> MedicalChatState:\n",
        "    if not state.get(\"messages\"):\n",
        "        print(\"ğŸ¤– MedBot: Welcome to MedBot! Ask me any medical question.\")\n",
        "    else:\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "        print(\"ğŸ¤– MedBot:\", last_msg.content)\n",
        "\n",
        "    user_input = input(\"ğŸ‘¤ User: \")\n",
        "\n",
        "    # Handle exit\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"goodbye\"}:\n",
        "        return {**state, \"finished\": True}\n",
        "\n",
        "    # Append new user message\n",
        "    messages = state.get(\"messages\", [])\n",
        "    return {**state, \"messages\": messages + [HumanMessage(content=user_input)], \"finished\": False}\n",
        "\n",
        "\n",
        "# âœ… Chatbot node with RAG fallback\n",
        "def chatbot_with_rag_fallback(state: MedicalChatState) -> MedicalChatState:\n",
        "    if not state[\"messages\"]:\n",
        "        return {**state, \"messages\": [AIMessage(content=\"Welcome to MedBot! Ask me any medical question.\")]}\n",
        "\n",
        "    query = state[\"messages\"][-1].content\n",
        "    retrieved_answer = retrieve_medical_answer.invoke(query)\n",
        "\n",
        "    if retrieved_answer:\n",
        "        return {**state, \"messages\": state[\"messages\"] + [AIMessage(content=retrieved_answer)]}\n",
        "    else:\n",
        "        # Placeholder fallback response\n",
        "        fallback_response = \"I'm sorry, I couldn't find a direct answer. Please consult a medical professional.\"\n",
        "        return {**state, \"messages\": state[\"messages\"] + [AIMessage(content=fallback_response)]}\n",
        "\n",
        "\n",
        "# âœ… Route control\n",
        "def route_from_chatbot(state: MedicalChatState) -> Literal[\"human\", \"chatbot\"]:\n",
        "    return \"human\"\n",
        "\n",
        "\n",
        "def decide_next_step(state: MedicalChatState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    if state.get(\"finished\", False):\n",
        "        print(\"ğŸ‘‹ Goodbye! Take care.\")\n",
        "        return END\n",
        "    return \"chatbot\"\n",
        "\n",
        "\n",
        "# âœ… Build the LangGraph\n",
        "graph_builder = StateGraph(MedicalChatState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_rag_fallback)\n",
        "\n",
        "# Define edges\n",
        "graph_builder.set_entry_point(\"human\")\n",
        "graph_builder.add_conditional_edges(\"human\", decide_next_step)\n",
        "graph_builder.add_conditional_edges(\"chatbot\", route_from_chatbot)\n",
        "\n",
        "# âœ… Compile and run the graph\n",
        "medical_chat_graph = graph_builder.compile()\n",
        "\n",
        "# âœ… Run the chatbot\n",
        "state = {\"messages\": [], \"finished\": False}\n",
        "medical_chat_graph.invoke(state)\n"
      ],
      "metadata": {
        "id": "QSB4LFCDuQ5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tesseract"
      ],
      "metadata": {
        "id": "6acCfpfLuSs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Set up Gemini API\n",
        "genai.configure(api_key=\"\")\n",
        "\n",
        "### ğŸ“Œ 1ï¸âƒ£ Define Medical Chatbot State ###\n",
        "class MedicalChatState(TypedDict):\n",
        "    messages: list\n",
        "    medical_report_text: str\n",
        "    finished: bool\n",
        "\n",
        "### ğŸ“Œ 2ï¸âƒ£ Extract Text from Image (OCR) ###\n",
        "def extract_text_from_image(image_path):\n",
        "    try:\n",
        "        if not os.path.exists(image_path):\n",
        "            return \"Error: Image file not found.\"\n",
        "\n",
        "        image = Image.open(image_path)\n",
        "        extracted_text = pytesseract.image_to_string(image).strip()\n",
        "\n",
        "        return extracted_text if extracted_text else \"No readable text found in the image.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error processing image: {str(e)}\"\n",
        "\n",
        "### ğŸ“Œ 3ï¸âƒ£ Gemini 1.5 Flash AI Response ###\n",
        "def chatbot_with_welcome_msg(state: MedicalChatState) -> MedicalChatState:\n",
        "    if state[\"messages\"]:\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "\n",
        "        # If medical text exists, AI should use it\n",
        "        medical_text = state.get(\"medical_report_text\", \"\")\n",
        "        user_query = last_msg[\"content\"]\n",
        "\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        prompt = f\"\"\"\n",
        "        You are a medical assistant AI. The extracted text from a patient's medical report is:\n",
        "\n",
        "        {medical_text}\n",
        "\n",
        "        The user has asked: \"{user_query}\"\n",
        "\n",
        "        Provide a clear and medically accurate response based on the report.\n",
        "        \"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        ai_response = AIMessage(content=response.text)\n",
        "    else:\n",
        "        ai_response = AIMessage(content=\"Welcome to the Medical Chatbot! Type 'img' to upload a medical report.\")\n",
        "\n",
        "    return state | {\"messages\": [ai_response]}\n",
        "\n",
        "### ğŸ“Œ 4ï¸âƒ£ Human Node with Image Handling ###\n",
        "def human_node(state: MedicalChatState) -> MedicalChatState:\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "    print(\"\\nModel:\", last_msg.content)\n",
        "\n",
        "    user_input = input(\"User (type text or enter 'img' to upload an image): \")\n",
        "\n",
        "    if user_input.lower() in {\"q\", \"quit\", \"goodbye\"}:\n",
        "        state[\"finished\"] = True\n",
        "        return state\n",
        "\n",
        "    # If the user wants to upload an image\n",
        "    if user_input.lower() == \"img\":\n",
        "        image_path = input(\"Enter the image path: \")\n",
        "        extracted_text = extract_text_from_image(image_path)\n",
        "\n",
        "        print(\"\\nExtracted Text:\\n\", extracted_text)\n",
        "\n",
        "        return state | {\"medical_report_text\": extracted_text, \"messages\": [{\"role\": \"user\", \"content\": \"Uploaded a medical report.\"}]}\n",
        "\n",
        "    return state | {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
        "\n",
        "### ğŸ“Œ 5ï¸âƒ£ Conditional Exit ###\n",
        "def maybe_exit_human_node(state: MedicalChatState) -> Literal[\"chatbot\", \"__end__\"]:\n",
        "    return END if state.get(\"finished\", False) else \"chatbot\"\n",
        "\n",
        "### ğŸ“Œ 6ï¸âƒ£ Build Conversation Flow ###\n",
        "graph_builder = StateGraph(MedicalChatState)\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", \"human\")\n",
        "graph_builder.add_conditional_edges(\"human\", maybe_exit_human_node)\n",
        "\n",
        "chat_with_human_graph = graph_builder.compile()\n",
        "\n",
        "### ğŸ“Œ 7ï¸âƒ£ Start Chatbot ###\n",
        "def main():\n",
        "    state = chat_with_human_graph.invoke({\"messages\": [], \"medical_report_text\": \"\"})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RAclbGkXuWWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
