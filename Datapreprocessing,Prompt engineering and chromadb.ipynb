{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI9PDk1tCKo5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "# Load dataset\n",
        "ds = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")\n",
        "data = ds['train']\n",
        "\n",
        "# Initialize preprocessing tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text.\"\"\"\n",
        "    if not isinstance(text, str) or text.strip() == \"\":  # Handle missing or empty values\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()  # Lowercasing\n",
        "    word_tokens = word_tokenize(text)  # Tokenize into words\n",
        "    filtered_words = [word for word in word_tokens if word not in stop_words]  # Remove stopwords\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]  # Lemmatization\n",
        "    return ' '.join(lemmatized_words)  # Join back into string\n",
        "\n",
        "# Process dataset (using correct column names)\n",
        "processed_data = []\n",
        "for entry in data:\n",
        "    processed_entry = {}\n",
        "\n",
        "    # Extract columns\n",
        "    qtype = entry.get('qtype', \"\").strip()  # Keep as is\n",
        "    question_text = entry.get('Question', \"\").strip()\n",
        "    answer_text = entry.get('Answer', \"\").strip()\n",
        "\n",
        "    # Preprocess question\n",
        "    if question_text:\n",
        "        cleaned_question = preprocess_text(question_text)\n",
        "        processed_entry['Question'] = cleaned_question\n",
        "        processed_entry['Question_Sentences'] = ' '.join(sent_tokenize(question_text))  # Keep original structure\n",
        "        processed_entry['Question_Words'] = ' '.join(word_tokenize(cleaned_question))\n",
        "\n",
        "    # Preprocess answer\n",
        "    if answer_text:\n",
        "        cleaned_answer = preprocess_text(answer_text)\n",
        "        processed_entry['Answer'] = cleaned_answer\n",
        "        processed_entry['Answer_Sentences'] = ' '.join(sent_tokenize(answer_text))\n",
        "        processed_entry['Answer_Words'] = ' '.join(word_tokenize(cleaned_answer))\n",
        "\n",
        "    processed_entry['qtype'] = qtype  # Keep original qtype\n",
        "\n",
        "    if processed_entry:\n",
        "        processed_data.append(processed_entry)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(\"processed_medquad.csv\", index=False)\n",
        "\n",
        "print(\"CSV file 'processed_medquad.csv' saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oES70pScK0u"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTYV8xOoHdII"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "# Load the processed dataset\n",
        "file_path = \"/content/processed_medquad.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure no NaN values in text columns\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Combine 'Question_Words' and 'Answer_Words' for analysis\n",
        "all_words = df[\"Question_Words\"].tolist() + df[\"Answer_Words\"].tolist()\n",
        "\n",
        "# Tokenize words for POS tagging\n",
        "tokenized_words = [word_tokenize(sentence) for sentence in all_words if isinstance(sentence, str)]\n",
        "flat_words = [word for sublist in tokenized_words for word in sublist]  # Flatten list\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = pos_tag(flat_words)\n",
        "\n",
        "print(\"The POS Tags are as follows\")\n",
        "print(pos_tags)\n",
        "# Count POS distribution\n",
        "pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "# Convert POS counts to a DataFrame for visualization\n",
        "pos_df = pd.DataFrame(pos_counts.items(), columns=[\"POS\", \"Count\"])\n",
        "pos_df = pos_df.sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# Plot POS distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=\"POS\", y=\"Count\", data=pos_df, palette=\"viridis\")\n",
        "plt.xlabel(\"Part of Speech (POS)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"POS Distribution in Processed MedQuad Dataset\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_words, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Extract word vectors for visualization\n",
        "words = list(word2vec_model.wv.index_to_key)\n",
        "word_vectors = word2vec_model.wv[words]\n",
        "\n",
        "# Reduce dimensionality using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "embedding_df = pd.DataFrame(word_vectors_2d, columns=[\"x\", \"y\"])\n",
        "embedding_df[\"word\"] = words\n",
        "\n",
        "# Plot t-SNE visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x=\"x\", y=\"y\", data=embedding_df, alpha=0.7)\n",
        "\n",
        "# Annotate some points\n",
        "for i, row in embedding_df.sample(30, random_state=42).iterrows():  # Show 30 random words\n",
        "    plt.text(row[\"x\"], row[\"y\"], row[\"word\"], fontsize=9)\n",
        "\n",
        "plt.title(\"t-SNE Visualization of Word Embeddings (Word2Vec)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isnFXS9oabTV"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from evaluate import load\n",
        "from deepeval.test_run import TestRun\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval import evaluate\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize Gemini client\n",
        "client = genai.Client(api_key=\"\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "def generate_medical_tot_prompt(question):\n",
        "    \"\"\"Generate a Tree-of-Thought (ToT) enhanced prompt for multi-path reasoning in medical queries.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert AI medical assistant trained on verified medical knowledge.\n",
        "You have access to a **structured medical dataset (MedQuad)** containing information about **diseases, symptoms, treatments, medications, and medical conditions**.\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "Use a Tree-of-Thought (ToT) approach to explore different reasoning paths before selecting the best response:\n",
        "\n",
        "1. **Branching:** Consider multiple possible explanations or answers related to the question (e.g., differential diagnoses, alternative treatments, symptom variations).\n",
        "2  **Outline:** Outline a tree of thought where you consider different possible approaches or branches\n",
        "3. **Evaluation:** Assess the validity, reliability, and potential risks of each possible answer.\n",
        "4. **Selection:** Choose the best explanation or recommendation based on evidence, medical guidelines, and logical reasoning.\n",
        "5. **Refinement:** Verify the final answer by cross-referencing medical facts and ensuring clarity.\n",
        "6. **Synthesize** the findings into a well-structured, medically accurate response.\n",
        "\n",
        "**Final Answer:**\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "\n",
        "def query_gemini(prompt, model=\"gemini-2.0-flash\"):\n",
        "    \"\"\"Query Gemini with a given prompt and return the response.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=[prompt],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Example usage\n",
        "\n",
        "print(\"Using TOT technique\")\n",
        "user_query = \"what are the symptoms for Parasites - Cysticercosis ?\"\n",
        "expected_answer=\"LCMV is most commonly recognized as causing neurological disease, as its name implies, though infection without symptoms or mild febrile illnesses are more common clinical manifestations. For infected persons who do become ill, onset of symptoms usually occurs 8-13 days after exposure to the virus as part of a biphasic febrile illness. This initial phase, which may last as long as a week, typically begins with any or all of the following symptoms: fever, malaise, lack of appetite, muscle aches, headache, nausea, and vomiting. Other symptoms appearing less frequently include sore throat, cough, joint pain, chest pain, testicular pain, and parotid (salivary gland) pain. Following a few days of recovery, a second phase of illness may occur. Symptoms may consist of meningitis (fever, headache, stiff neck, etc. ), encephalitis (drowsiness, confusion, sensory disturbances, and/or motor abnormalities, such as paralysis), or meningoencephalitis (inflammation of both the brain and meninges). LCMV has also been known to cause acute hydrocephalus (increased fluid on the brain), which often requires surgical shunting to relieve increased intracranial pressure. In rare instances, infection results in myelitis (inflammation of the spinal cord) and presents with symptoms such as muscle weakness, paralysis, or changes in body sensation. An association between LCMV infection and myocarditis (inflammation of the heart muscles) has been suggested. Previous observations show that most patients who develop aseptic meningitis or encephalitis due to LCMV survive. No chronic infection has been described in humans, and after the acute phase of illness, the virus is cleared from the body. However, as in all infections of the central nervous system, particularly encephalitis, temporary or permanent neurological damage is possible. Nerve deafness and arthritis have been reported. Women who become infected with LCMV during pregnancy may pass the infection on to the fetus. Infections occurring during the first trimester may result in fetal death and pregnancy termination, while in the second and third trimesters, birth defects can develop. Infants infected In utero can have many serious and permanent birth defects, including vision problems, mental retardation, and hydrocephaly (water on the brain). Pregnant women may recall a flu-like illness during pregnancy, or may not recall any illness. LCM is usually not fatal. In general, mortality is less than 1%.\"\n",
        "\n",
        "tot_prompt = generate_medical_tot_prompt(user_query)\n",
        "response = query_gemini(tot_prompt)\n",
        "print(response)\n",
        "\n",
        "\n",
        "generated_answer = response.strip()\n",
        "reference_answer = expected_answer.strip()\n",
        "\n",
        "# Tokenize\n",
        "reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"BLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ROUGE\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[reference_answer])\n",
        "print(\"ROUGE Score:\", rouge_score)\n",
        "\n",
        "# F1 Score (word-level token overlap)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "f1 = f1_from_tokens(reference_tokens, generated_tokens)\n",
        "print(\"F1 Score:\", round(f1, 4))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa4Cbdgpt_b1"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "# Initialize Gemini client\n",
        "client = genai.Client(api_key=\"\")\n",
        "\n",
        "def generate_medical_cot_prompt(question):\n",
        "    \"\"\"Generate a Chain-of-Thought (CoT) enhanced prompt for structured reasoning in medical queries.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert AI medical assistant trained on verified medical knowledge.\n",
        "You have access to a **structured medical dataset (MedQuad)** containing information about **diseases, symptoms, treatments, medications, and medical conditions**.\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "Think step-by-step before answering:\n",
        "1. **Identify** what the user is asking (e.g., is it about a disease, symptoms, treatment, medication, or diagnosis?).\n",
        "2. **Outline** outline the chain of thought before generating\n",
        "2. **Determine** the best way to retrieve the answer using the MedQuad dataset.\n",
        "3. **Extract** relevant medical facts and structure them logically.\n",
        "4. **Interpret** the findings into a clear and medically accurate response.\n",
        "5.**Generate** the final response not the code\n",
        "\n",
        "**Final Answer:**\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def query_gemini(prompt, model=\"gemini-2.0-flash\"):\n",
        "    \"\"\"Query Gemini with a given prompt and return the response.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=[prompt],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Example usage\n",
        "\n",
        "print(\"Using COT technique\")\n",
        "user_query = \"what are the symptoms for Parasites - Cysticercosis ?\"\n",
        "expected_answer=\"LCMV is most commonly recognized as causing neurological disease, as its name implies, though infection without symptoms or mild febrile illnesses are more common clinical manifestations. For infected persons who do become ill, onset of symptoms usually occurs 8-13 days after exposure to the virus as part of a biphasic febrile illness. This initial phase, which may last as long as a week, typically begins with any or all of the following symptoms: fever, malaise, lack of appetite, muscle aches, headache, nausea, and vomiting. Other symptoms appearing less frequently include sore throat, cough, joint pain, chest pain, testicular pain, and parotid (salivary gland) pain. Following a few days of recovery, a second phase of illness may occur. Symptoms may consist of meningitis (fever, headache, stiff neck, etc. ), encephalitis (drowsiness, confusion, sensory disturbances, and/or motor abnormalities, such as paralysis), or meningoencephalitis (inflammation of both the brain and meninges). LCMV has also been known to cause acute hydrocephalus (increased fluid on the brain), which often requires surgical shunting to relieve increased intracranial pressure. In rare instances, infection results in myelitis (inflammation of the spinal cord) and presents with symptoms such as muscle weakness, paralysis, or changes in body sensation. An association between LCMV infection and myocarditis (inflammation of the heart muscles) has been suggested. Previous observations show that most patients who develop aseptic meningitis or encephalitis due to LCMV survive. No chronic infection has been described in humans, and after the acute phase of illness, the virus is cleared from the body. However, as in all infections of the central nervous system, particularly encephalitis, temporary or permanent neurological damage is possible. Nerve deafness and arthritis have been reported. Women who become infected with LCMV during pregnancy may pass the infection on to the fetus. Infections occurring during the first trimester may result in fetal death and pregnancy termination, while in the second and third trimesters, birth defects can develop. Infants infected In utero can have many serious and permanent birth defects, including vision problems, mental retardation, and hydrocephaly (water on the brain). Pregnant women may recall a flu-like illness during pregnancy, or may not recall any illness. LCM is usually not fatal. In general, mortality is less than 1%.\"\n",
        "\n",
        "cot_prompt = generate_medical_cot_prompt(user_query)\n",
        "response = query_gemini(cot_prompt)\n",
        "print(response)\n",
        "\n",
        "generated_answer = response.strip()\n",
        "reference_answer = expected_answer.strip()\n",
        "\n",
        "# Tokenize\n",
        "reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"BLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ROUGE\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[reference_answer])\n",
        "print(\"ROUGE Score:\", rouge_score)\n",
        "\n",
        "# F1 Score (word-level token overlap)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "f1 = f1_from_tokens(reference_tokens, generated_tokens)\n",
        "print(\"F1 Score:\", round(f1, 4))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYUAABVE0q-c"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "# Initialize Gemini client\n",
        "client = genai.Client(api_key=\"\")\n",
        "\n",
        "def generate_medical_got_prompt(question):\n",
        "    \"\"\"Generate a Graph-of-Topics (GOT) enhanced prompt for structured reasoning in medical queries.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert AI medical assistant trained on verified medical knowledge.\n",
        "\n",
        "You have access to a **structured medical dataset (MedQuad)** containing information about **diseases, symptoms, treatments, medications, and medical conditions**.\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "Think using a **Graph-of-Thought (GoT)** approach before answering:\n",
        "\n",
        "1. **Identify** the key medical entity in the question (e.g., disease, symptom, treatment, medication, condition).\n",
        "2. **Map Relationships** by constructing a knowledge graph:\n",
        "   - **Central Node:** Define the core entity (e.g., disease, symptom).\n",
        "   - **Connected Nodes:** Identify related concepts (e.g., symptoms, causes, treatments, medications).\n",
        "   - **Edges:** Define the relationships between nodes (e.g., \"causes\", \"treated with\", \"associated with\").\n",
        "3. **Retrieve** relevant information from the MedQuad dataset using the graph structure.\n",
        "4. **Extract** medical knowledge as structured relationships.\n",
        "5. **Generate** a final answer based on the graph generate as a text and not in json format, ensuring clarity and medical accuracy.\n",
        "\n",
        "**Graph Representation:**\n",
        "**Final Answer:**\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def query_gemini(prompt, model=\"gemini-2.0-flash\"):\n",
        "    \"\"\"Query Gemini with a given prompt and return the response.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=[prompt],\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Example usage\n",
        "\n",
        "print(\"Using GOT technique\")\n",
        "user_query = \"what are the symptoms for Parasites - Cysticercosis ?\"\n",
        "expected_answer=\"LCMV is most commonly recognized as causing neurological disease, as its name implies, though infection without symptoms or mild febrile illnesses are more common clinical manifestations. For infected persons who do become ill, onset of symptoms usually occurs 8-13 days after exposure to the virus as part of a biphasic febrile illness. This initial phase, which may last as long as a week, typically begins with any or all of the following symptoms: fever, malaise, lack of appetite, muscle aches, headache, nausea, and vomiting. Other symptoms appearing less frequently include sore throat, cough, joint pain, chest pain, testicular pain, and parotid (salivary gland) pain. Following a few days of recovery, a second phase of illness may occur. Symptoms may consist of meningitis (fever, headache, stiff neck, etc. ), encephalitis (drowsiness, confusion, sensory disturbances, and/or motor abnormalities, such as paralysis), or meningoencephalitis (inflammation of both the brain and meninges). LCMV has also been known to cause acute hydrocephalus (increased fluid on the brain), which often requires surgical shunting to relieve increased intracranial pressure. In rare instances, infection results in myelitis (inflammation of the spinal cord) and presents with symptoms such as muscle weakness, paralysis, or changes in body sensation. An association between LCMV infection and myocarditis (inflammation of the heart muscles) has been suggested. Previous observations show that most patients who develop aseptic meningitis or encephalitis due to LCMV survive. No chronic infection has been described in humans, and after the acute phase of illness, the virus is cleared from the body. However, as in all infections of the central nervous system, particularly encephalitis, temporary or permanent neurological damage is possible. Nerve deafness and arthritis have been reported. Women who become infected with LCMV during pregnancy may pass the infection on to the fetus. Infections occurring during the first trimester may result in fetal death and pregnancy termination, while in the second and third trimesters, birth defects can develop. Infants infected In utero can have many serious and permanent birth defects, including vision problems, mental retardation, and hydrocephaly (water on the brain). Pregnant women may recall a flu-like illness during pregnancy, or may not recall any illness. LCM is usually not fatal. In general, mortality is less than 1%.\"\n",
        "\n",
        "cot_prompt = generate_medical_got_prompt(user_query)\n",
        "response = query_gemini(cot_prompt)\n",
        "print(response)\n",
        "\n",
        "\n",
        "generated_answer = response.strip()\n",
        "reference_answer = expected_answer.strip()\n",
        "\n",
        "# Tokenize\n",
        "reference_tokens = nltk.word_tokenize(reference_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"BLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ROUGE\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[reference_answer])\n",
        "print(\"ROUGE Score:\", rouge_score)\n",
        "\n",
        "# F1 Score (word-level token overlap)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "f1 = f1_from_tokens(reference_tokens, generated_tokens)\n",
        "print(\"F1 Score:\", round(f1, 4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zMuB66_ayR-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# ✅ Load and Clean Dataset\n",
        "csv_path = \"/content/processed_medquad.csv\"  # Adjust if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "df = df.dropna(subset=[\"Question_Sentences\", \"Answer_Sentences\"])\n",
        "\n",
        "# ✅ Convert to LangChain Documents\n",
        "docs_to_add = [\n",
        "    Document(\n",
        "        page_content=str(row[\"Question_Sentences\"]).strip(),\n",
        "        metadata={\n",
        "            \"answer\": str(row[\"Answer_Sentences\"]).strip(),\n",
        "            \"qtype\": str(row[\"qtype\"]).strip()\n",
        "        }\n",
        "    )\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "\n",
        "# ✅ Initialize Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ✅ Initialize Chroma Vector Store\n",
        "vector_store = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# ✅ Function to Add Documents in Batches\n",
        "def batch_add_documents(vector_store, docs, batch_size=5000):\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        vector_store.add_documents(batch)\n",
        "        print(f\"✅ Added batch {i//batch_size + 1} with {len(batch)} documents\")\n",
        "\n",
        "# ✅ Run Batched Insert\n",
        "batch_add_documents(vector_store, docs_to_add)\n",
        "\n",
        "print(\"✅ Medical QA dataset successfully added to Chroma vector store!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3tXlM0EgBBu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import sacrebleu\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from evaluate import load\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ✅ Load tokenizer for nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ✅ Load ChromaDB (Vector Store)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
        "\n",
        "# ✅ Load FLAN-T5 model & tokenizer (for answer generation)\n",
        "model_name = \"google/flan-t5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_medical_cot_prompt(question, retrieved_answer):\n",
        "    \"\"\"\n",
        "    Generate a Chain-of-Thought (CoT) enhanced prompt for structured reasoning in medical queries.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an expert AI medical assistant trained on verified medical knowledge.\n",
        "You have access to a **structured medical dataset (MedQuad)** containing information about **diseases, symptoms, treatments, medications, and medical conditions**.\n",
        "\n",
        "### **User Question:**\n",
        "{question}\n",
        "\n",
        "### **Retrieved Medical Information:**\n",
        "{retrieved_answer}\n",
        "\n",
        "### **Think step-by-step before answering:**\n",
        "1. **Identify** what the user is asking (e.g., is it about a disease, symptoms, treatment, medication, or diagnosis?).\n",
        "2. **Outline** the chain of thought before generating.\n",
        "3. **Determine** the best way to retrieve the answer using the MedQuad dataset.\n",
        "4. **Extract** relevant medical facts and structure them logically.\n",
        "5. **Interpret** the findings into a clear and medically accurate response.\n",
        "6. **Generate** the final answer.\n",
        "\n",
        "### **Final Answer:**\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def generate_answer_with_flan(question, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the most similar question from ChromaDB and generates an answer using FLAN-T5\n",
        "    with a Chain-of-Thought (CoT) enhanced prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # ✅ Retrieve similar questions from ChromaDB\n",
        "    search_results = vector_store.similarity_search(question, k=top_k)\n",
        "\n",
        "    if not search_results:\n",
        "        return \"Sorry, I couldn't find relevant information.\"\n",
        "\n",
        "    # ✅ Use the best-matching question as context\n",
        "    best_match = search_results[0]\n",
        "    retrieved_question = best_match.page_content\n",
        "    retrieved_answer = best_match.metadata.get(\"answer\", \"No answer found.\")\n",
        "\n",
        "    # ✅ Apply CoT Prompting\n",
        "    cot_prompt = generate_medical_cot_prompt(retrieved_question, retrieved_answer)\n",
        "\n",
        "    # ✅ Generate answer using FLAN-T5\n",
        "    inputs = tokenizer(cot_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_length=1024,\n",
        "            min_length=100,\n",
        "            num_beams=3,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_answer, retrieved_answer\n",
        "\n",
        "# ✅ Example usage\n",
        "query = \"How to diagnose Crimean-Congo Hemorrhagic Fever (CCHF)?\"\n",
        "expected_answer = \"\"\"\n",
        "Laboratory tests that are used to diagnose CCHF include antigen-capture enzyme-linked immunosorbent assay (ELISA), real time polymerase chain reaction (RT-PCR), virus isolation attempts, and detection of antibody by ELISA (IgG and IgM). Laboratory diagnosis of a patient with a clinical history compatible with CCHF can be made during the acute phase of the disease by using the combination of detection of the viral antigen (ELISA antigen capture), viral RNA sequence (RT-PCR) in the blood or in tissues collected from a fatal case and virus isolation. Immunohistochemical staining can also show evidence of viral antigen in formalin-fixed tissues. Later in the course of the disease, in people surviving, antibodies can be found in the blood. But antigen, viral RNA and virus are no more present and detectable.\n",
        "\"\"\".strip()\n",
        "\n",
        "generated_answer, retrieved_answer = generate_answer_with_flan(query)\n",
        "print(\"Generated Answer:\\n\", generated_answer)\n",
        "print(\"\\nReference (Expected) Answer:\\n\", expected_answer)\n",
        "\n",
        "# ✅ Tokenize answers\n",
        "reference_tokens = nltk.word_tokenize(expected_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# ✅ SacreBLEU Score\n",
        "sacrebleu_score = sacrebleu.corpus_bleu([generated_answer], [[expected_answer]])\n",
        "print(\"\\nSacreBLEU Score:\", round(sacrebleu_score.score, 2))\n",
        "\n",
        "# ✅ BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"\\nBLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ✅ ROUGE Score\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
        "print(\"\\nROUGE Score:\", rouge_score)\n",
        "\n",
        "# ✅ F1 Score (word overlap, not the sklearn f1)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "f1 = f1_from_tokens(reference_tokens, generated_tokens)\n",
        "print(\"F1 Score:\", round(f1, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkciAYGX2uMO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import sacrebleu\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from evaluate import load\n",
        "\n",
        "# Ensure nltk punkt tokenizer is available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ✅ Load ChromaDB (Vector Store)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
        "\n",
        "# ✅ Load FLAN-T5 model & tokenizer (for answer generation)\n",
        "model_name = \"google/flan-t5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_medical_tot_prompt(question, retrieved_answer):\n",
        "    \"\"\"Tree-of-Thought (ToT) Prompt for Multi-Step Reasoning\"\"\"\n",
        "    return (\n",
        "        \"Please solve the following problem by exploring multiple lines of reasoning. \"\n",
        "        \"Outline a tree of thought where you consider different possible approaches or branches, \"\n",
        "        \"and then converge on the best solution with your final answer.\\n\\n\"\n",
        "        f\"**User Question:** {question}\\n\\n\"\n",
        "        f\"**Retrieved Medical Information:** {retrieved_answer}\\n\\n\"\n",
        "        \"Now, apply structured reasoning and provide the best medically accurate response.\"\n",
        "    )\n",
        "\n",
        "def generate_answer_with_flan(question, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieves the most similar question from ChromaDB and generates an answer using FLAN-T5\n",
        "    with a Tree-of-Thought (ToT) prompt.\n",
        "    \"\"\"\n",
        "    search_results = vector_store.similarity_search(question, k=top_k)\n",
        "    if not search_results:\n",
        "        return \"Sorry, I couldn't find relevant information.\", \"No reference answer available.\"\n",
        "\n",
        "    best_match = search_results[0]\n",
        "    retrieved_question = best_match.page_content\n",
        "    retrieved_answer = best_match.metadata.get(\"answer\", \"No answer found.\")\n",
        "\n",
        "    # ✅ Apply ToT Prompting\n",
        "    tot_prompt = generate_medical_tot_prompt(retrieved_question, retrieved_answer)\n",
        "    inputs = tokenizer(tot_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_length=1024,\n",
        "            min_length=100,\n",
        "            num_beams=3,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_answer, retrieved_answer\n",
        "\n",
        "# ✅ Example usage\n",
        "query = \"How to diagnose Crimean-Congo Hemorrhagic Fever (CCHF) ?\"\n",
        "expected_answer = \"\"\"Laboratory tests that are used to diagnose CCHF include antigen-capture enzyme-linked immunosorbent assay (ELISA), real time polymerase chain reaction (RT-PCR), virus isolation attempts, and detection of antibody by ELISA (IgG and IgM). Laboratory diagnosis of a patient with a clinical history compatible with CCHF can be made during the acute phase of the disease by using the combination of detection of the viral antigen (ELISA antigen capture), viral RNA sequence (RT-PCR) in the blood or in tissues collected from a fatal case and virus isolation. Immunohistochemical staining can also show evidence of viral antigen in formalin-fixed tissues. Later in the course of the disease, in people surviving, antibodies can be found in the blood. But antigen, viral RNA and virus are no more present and detectable.\"\"\"\n",
        "\n",
        "# ✅ Generate answer\n",
        "generated_answer, retrieved_answer = generate_answer_with_flan(query)\n",
        "print(\"Generated Answer:\\n\", generated_answer)\n",
        "print(\"\\nReference Answer:\\n\", expected_answer)\n",
        "\n",
        "# ✅ Evaluation\n",
        "reference_tokens = nltk.word_tokenize(expected_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# ✅ SacreBLEU\n",
        "sacrebleu_score = sacrebleu.corpus_bleu([generated_answer], [[expected_answer]])\n",
        "print(\"\\nSacreBLEU Score:\", round(sacrebleu_score.score, 2))\n",
        "\n",
        "# ✅ BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"\\nBLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ✅ ROUGE\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
        "print(\"\\nROUGE Score:\", rouge_score)\n",
        "\n",
        "# ✅ F1 Score (word-level)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "f1 = f1_from_tokens(reference_tokens, generated_tokens)\n",
        "print(\"F1 Score:\", round(f1, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZYdwIEocrrp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from evaluate import load\n",
        "import sacrebleu\n",
        "\n",
        "# Make sure NLTK data is downloaded\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# ✅ Set up Gemini API Key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"  # 🔐 Replace with your real key\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# ✅ Load ChromaDB\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
        "\n",
        "# ✅ Load Gemini model\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "\n",
        "def generate_medical_got_prompt(question, retrieved_answer):\n",
        "    return (\n",
        "        f\"Build a structured medical knowledge response for the following question.\\n\\n\"\n",
        "        f\"**User Question:** {question}\\n\\n\"\n",
        "        f\"**Retrieved Medical Information:** {retrieved_answer}\\n\\n\"\n",
        "        \"Identify key medical concepts, explain their relationships, and provide an informative yet concise response.\"\n",
        "    )\n",
        "\n",
        "def generate_answer_with_gemini(question, top_k=3):\n",
        "    search_results = vector_store.similarity_search(question, k=top_k)\n",
        "    if not search_results:\n",
        "        return \"Sorry, I couldn't find relevant medical information.\"\n",
        "\n",
        "    best_match = search_results[0]\n",
        "    retrieved_answer = best_match.metadata.get(\"answer\", \"No answer found.\")\n",
        "\n",
        "    got_prompt = generate_medical_got_prompt(question, retrieved_answer)\n",
        "    gemini_response = model.invoke(got_prompt)\n",
        "\n",
        "    return gemini_response.content.strip()\n",
        "\n",
        "# ✅ User query & gold-standard expected answer\n",
        "query = \"How to diagnose Crimean-Congo Hemorrhagic Fever (CCHF) ?\"\n",
        "expected_answer = \"\"\"Laboratory tests that are used to diagnose CCHF include antigen-capture enzyme-linked immunosorbent assay (ELISA), real time polymerase chain reaction (RT-PCR), virus isolation attempts, and detection of antibody by ELISA (IgG and IgM). Laboratory diagnosis of a patient with a clinical history compatible with CCHF can be made during the acute phase of the disease by using the combination of detection of the viral antigen (ELISA antigen capture), viral RNA sequence (RT-PCR) in the blood or in tissues collected from a fatal case and virus isolation. Immunohistochemical staining can also show evidence of viral antigen in formalin-fixed tissues. Later in the course of the disease, in people surviving, antibodies can be found in the blood. But antigen, viral RNA and virus are no more present and detectable.\"\"\"\n",
        "\n",
        "generated_answer = generate_answer_with_gemini(query)\n",
        "print(\"🔹 Generated Answer:\\n\", generated_answer)\n",
        "print(\"\\n🔹 Expected Answer (Ground Truth):\\n\", expected_answer)\n",
        "\n",
        "# ✅ Tokenization\n",
        "expected_tokens = nltk.word_tokenize(expected_answer.lower())\n",
        "generated_tokens = nltk.word_tokenize(generated_answer.lower())\n",
        "\n",
        "# ✅ BLEU Scores\n",
        "smoothing = SmoothingFunction().method1\n",
        "print(\"\\n🔹 BLEU Scores:\")\n",
        "print(\"BLEU-1:\", sentence_bleu([expected_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-2:\", sentence_bleu([expected_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-3:\", sentence_bleu([expected_tokens], generated_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing))\n",
        "print(\"BLEU-4:\", sentence_bleu([expected_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing))\n",
        "\n",
        "# ✅ ROUGE Score\n",
        "rouge = load(\"rouge\")\n",
        "rouge_score = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
        "print(\"\\n🔹 ROUGE Score:\", rouge_score)\n",
        "\n",
        "# ✅ F1 Score (word-level token overlap)\n",
        "def f1_from_tokens(ref_tokens, gen_tokens):\n",
        "    ref_set = set(ref_tokens)\n",
        "    gen_set = set(gen_tokens)\n",
        "    true_positives = len(ref_set & gen_set)\n",
        "    precision = true_positives / len(gen_set) if gen_set else 0\n",
        "    recall = true_positives / len(ref_set) if ref_set else 0\n",
        "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "f1 = f1_from_tokens(expected_tokens, generated_tokens)\n",
        "print(\"🔹 F1 Score:\", round(f1, 4))\n",
        "\n",
        "# ✅ SacreBLEU\n",
        "sacrebleu_score = sacrebleu.corpus_bleu([generated_answer], [[expected_answer]])\n",
        "print(\"🔹 SacreBLEU Score:\", round(sacrebleu_score.score, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_b0ys92y1fb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
